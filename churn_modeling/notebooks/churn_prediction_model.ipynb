import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score
)
from sklearn.linear_model import LogisticRegression

from xgboost import XGBClassifier
from catboost import CatBoostClassifier
import joblib

# =========================================================
# 1. LOAD & PREPARE DATA
# =========================================================
df = pd.read_csv("../data/processed/ravenstack_churn_ml_dataset_90day.csv")

target_col = "churn_flag_y"
y = df[target_col]

cols_to_drop = [
    target_col,
    "churn_flag_x",
    "account_id",
    "account_name",
    "feedback_text",
    "signup_date",
    "churn_date"
]
cols_to_drop = [c for c in cols_to_drop if c in df.columns]

X = df.drop(columns=cols_to_drop)

# One-hot encode (XGBoost needs this)
X_encoded = pd.get_dummies(X, drop_first=True).fillna(0)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y,
    test_size=0.3,
    stratify=y,
    random_state=42
)

# =========================================================
# 2. FEATURE SELECTION USING BASE XGBOOST
# =========================================================
scale_pos = (y_train == 0).sum() / (y_train == 1).sum()

xgb_base = XGBClassifier(
    n_estimators=400,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=1,
    scale_pos_weight=scale_pos,
    eval_metric="auc",
    random_state=42
)

xgb_base.fit(X_train, y_train)

importances = pd.Series(xgb_base.feature_importances_, index=X_train.columns)
selected_features = importances[importances > 0.002].index

X_train_fs = X_train[selected_features]
X_test_fs  = X_test[selected_features]

print(f"Selected {len(selected_features)} features out of {len(X_train.columns)}")

# =========================================================
# 3. FINAL MODELS (XGBOOST + CATBOOST)
# =========================================================
xgb_fs = XGBClassifier(
    n_estimators=600,
    learning_rate=0.2,
    max_depth=6,
    subsample=1.0,
    colsample_bytree=0.6,
    min_child_weight=1,
    scale_pos_weight=scale_pos,
    eval_metric="auc",
    random_state=42
)
xgb_fs.fit(X_train_fs, y_train)

cb_fs = CatBoostClassifier(
    iterations=600,
    learning_rate=0.05,
    depth=8,
    loss_function="Logloss",
    eval_metric="AUC",
    class_weights=[1, 4],
    verbose=False
)
cb_fs.fit(X_train_fs, y_train)

# =========================================================
# 4. SOFT-VOTING ENSEMBLE
# =========================================================
xgb_proba = xgb_fs.predict_proba(X_test_fs)[:, 1]
cb_proba  = cb_fs.predict_proba(X_test_fs)[:, 1]

ensemble_proba = (0.6 * xgb_proba) + (0.4 * cb_proba)

# =========================================================
# 5. PROBABILITY CALIBRATION
# =========================================================
xgb_train_proba = xgb_fs.predict_proba(X_train_fs)[:, 1]
cb_train_proba  = cb_fs.predict_proba(X_train_fs)[:, 1]
ensemble_train_proba = (0.6 * xgb_train_proba) + (0.4 * cb_train_proba)

calibrator = LogisticRegression()
calibrator.fit(ensemble_train_proba.reshape(-1, 1), y_train)

ensemble_proba_cal = calibrator.predict_proba(ensemble_proba.reshape(-1, 1))[:, 1]

# =========================================================
# 6. THRESHOLD TUNING (FINAL)
# =========================================================
thresholds = np.arange(0.01, 0.50, 0.01)
best_f1 = -1
best_threshold = 0.10

for t in thresholds:
    preds = (ensemble_proba_cal >= t).astype(int)
    f1 = f1_score(y_test, preds)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = t

print("\nBEST THRESHOLD =", round(best_threshold, 3))
print("Best F1 =", round(best_f1, 3))

# =========================================================
# 7. FINAL MODEL EVALUATION
# =========================================================
y_pred_final = (ensemble_proba_cal >= best_threshold).astype(int)

print("\n==== FINAL MODEL REPORT ====")
print("Threshold =", best_threshold)
print(classification_report(y_test, y_pred_final))
print("ROC-AUC:", roc_auc_score(y_test, ensemble_proba_cal))
print(confusion_matrix(y_test, y_pred_final))

# =========================================================
# 8. SAVE FINAL MODELS
# =========================================================
joblib.dump(xgb_fs, "../models/xgb_final.pkl")
joblib.dump(cb_fs, "../models/catboost_final.pkl")
joblib.dump(calibrator, "../models/calibration_final.pkl")
joblib.dump(list(selected_features), "../models/selected_features.pkl")

print("\nðŸŽ‰ Final models saved successfully.")

# Load retained customers
retained_df = df[df["churn_flag_y"] == 0].reset_index(drop=True)

Xr = retained_df.drop(columns=[
    "churn_flag_y","churn_flag_x","account_id","account_name",
    "feedback_text","signup_date","churn_date"
], errors="ignore")


Xr_enc = pd.get_dummies(Xr, drop_first=False)


for col in selected_features:
    if col not in Xr_enc.columns:
        Xr_enc[col] = 0 

Xr_enc = Xr_enc[selected_features]
Xr_use = Xr_enc.copy()

# Predictions
p_xgb = xgb_fs.predict_proba(Xr_use)[:, 1]
p_cb  = cb_fs.predict_proba(Xr_use)[:, 1]
p_ens = 0.6*p_xgb + 0.4*p_cb
final_p = calibrator.predict_proba(p_ens.reshape(-1,1))[:,1]

import xgboost as xgb

dtest = xgb.DMatrix(
    Xr_use.values,
    feature_names=list(Xr_use.columns.astype(str))
)

importance_matrix = xgb_fs.get_booster().predict(
    dtest, pred_contribs=True
)

feature_names = np.array(list(Xr_use.columns.astype(str)))


def auto_clean_feature_name(raw):
    """Convert any raw ML feature name into readable English."""
    name = raw.replace("last3_", "recent ")
    name = name.replace("avg_", "average ")
    name = name.replace("bucket_", "")
    name = name.replace("_", " ")

    # improve wording
    name = name.replace("Unhappy", "negative sentiment")
    name = name.replace("Neutral", "neutral sentiment")
    name = name.replace("Satisfied", "positive sentiment")
    name = name.replace("Increase", "increasing")
    name = name.replace("Stable", "stable")

    # Replace plan names cleanly
    import re
    name = re.sub(r"\b(Pro|Basic|Enterprise)\b", r"\1 plan", name)

    return name.strip().capitalize()


def explain_feature(feature, value):
    """Human-quality explanation for each churn driver."""

    # -----------------------------
    # LONG-TERM SATISFACTION
    # -----------------------------
    if feature == "satisfaction_avg":
        if value <= 2:
            return "long-term satisfaction scores are historically low, a strong churn predictor"
        elif value <= 4:
            return "long-term satisfaction is moderate"
        else:
            return "long-term satisfaction is strong"

    # -----------------------------
    # RECENT (LAST 3M) SATISFACTION
    # -----------------------------
    if feature == "last3_satisfaction_avg":
        if value <= 1:
            return "recent satisfaction sentiment has deteriorated sharply"
        elif value <= 3:
            return "recent satisfaction levels are weakening"
        else:
            return "recent satisfaction appears stable"

    if feature == "satisfaction_bucket_Unhappy":
        return "customer sentiment is consistently negative, a strong churn predictor"

    if feature == "satisfaction_bucket_Neutral":
        return "customer sentiment appears neutral and may indicate disengagement risk"

    if feature == "satisfaction_bucket_Satisfied":
        return "customer sentiment is positive, supporting retention stability"

    # -----------------------------
    # LONG-TERM USAGE
    # -----------------------------
    if feature == "usage_avg_min":
        if value < 5:
            return f"average session duration is low ({value:.2f} min), reflecting weak product engagement"
        elif value < 10:
            return f"average session duration is moderate ({value:.2f} min)"
        else:
            return f"average session duration is strong ({value:.2f} min)"

    # -----------------------------
    # SHORT-TERM USAGE (LAST 3M)
    # -----------------------------
    if feature == "last3_usage_avg_min":
        if value < 1:
            return "recent usage intensity has dropped significantly"
        elif value < 3:
            return "recent usage is below typical engagement levels"
        else:
            return "short-term usage appears stable"

    # -----------------------------
    # LAST 30-DAY USAGE COUNTS
    # -----------------------------
    if feature == "usage_last_30_days":
        if value < 10:
            return f"recent activity levels are low ({value} actions in last 30 days)"
        else:
            return f"recent activity levels indicate healthy engagement ({value} actions)"

    # -----------------------------
    # SUPPORT TICKETS
    # -----------------------------
    if feature == "ticket_count":
        if value >= 5:
            return f"support ticket volume is elevated ({value} tickets), indicating recurring issues"
        elif value >= 2:
            return f"moderate support request volume ({value} tickets)"
        else:
            return f"minimal support requests ({value} tickets)"

    if feature == "last3_ticket_bucket_Increase":
        return "recent support activity has increased sharply, indicating unresolved issues"

    if feature == "last3_ticket_bucket_Stable":
        return "support ticket activity has remained stable recently"

    # -----------------------------
    # INDUSTRY
    # -----------------------------
    if feature.startswith("industry_"):
        industry = feature.replace("industry_", "")
        return f"customer belongs to the {industry} industry segment, which exhibits distinct retention behavior"

    # -----------------------------
    # PLAN TIERS
    # -----------------------------
    if feature.startswith("plan_tier_"):
        tier = feature.replace("plan_tier_", "")
        return f"customer is on the {tier} plan tier, where churn often reflects value alignment and product utilization levels"

    # -----------------------------
    # ACQUISITION SOURCE
    # -----------------------------
    if feature == "referral_source_event":
        return "customer was acquired via event-based channels, which historically show higher churn volatility"

    if feature == "referral_source_organic":
        return "customer was acquired organically, a cohort associated with higher retention"

    if feature == "referral_source_partner":
        return "customer was acquired through partner channels, where churn depends on integration quality"

    # -----------------------------
    # TRIAL USERS
    # -----------------------------
    if feature == "is_trial":
        return "customer is currently in the trial phase, a stage characterized by high churn variability"

    # -----------------------------
    # FALLBACK â€” CLEAN NAME
    # -----------------------------
    return auto_clean_feature_name(feature)

import re
reasons = []

risk_drivers = []
retention_protectors = []

for i in range(len(Xr_use)):
    contribs = importance_matrix[i][:-1]
    vals = Xr_use.iloc[i]

    # sort by absolute contribution
    sorted_idx = np.argsort(np.abs(contribs))[::-1]

    drivers = []
    protectors = []

    for idx in sorted_idx:
        feature = feature_names[idx]
        value = vals.get(feature, None)
        explanation = explain_feature(feature, value)

        if contribs[idx] > 0 and len(drivers) < 3:
            drivers.append(explanation)

        if contribs[idx] < 0 and len(protectors) < 3:
            protectors.append(explanation)

        if len(drivers) == 3 and len(protectors) == 3:
            break

    risk_drivers.append("; ".join(drivers))
    retention_protectors.append("; ".join(protectors))

	
result = pd.DataFrame({
    "account_id": retained_df["account_id"],
    "churn_probability": final_p,
})

result["churn_probability_percent"] = (
    (result["churn_probability"] * 100).round(1).astype(str) + "%"
)

final_reasons = []
for exp_list in reasons:
    if isinstance(exp_list, list):
        final_reasons.append("; ".join(exp_list))
    else:
        final_reasons.append(str(exp_list))

# --- Final output ---
result = pd.DataFrame({
    "account_id": retained_df["account_id"],
    "churn_probability": final_p,
})

result["churn_probability_percent"] = (
    (result["churn_probability"] * 100).round(1).astype(str) + "%"
)

result["risk_drivers"] = risk_drivers
result["retention_protectors"] = retention_protectors

def risk_label(p):
    if p > 0.70: return "High Risk"
    if p > 0.40: return "Medium Risk"
    return "Low Risk"

result["risk_level"] = result["churn_probability"].apply(risk_label)

result["explanation"] = result.apply(
    lambda row: (
        f"Predicted churn risk is {row['churn_probability_percent']}. "
        f"Key risk drivers: {row['risk_drivers']}. "
        f"Retention signals: {row['retention_protectors']}."
    ),
    axis=1
)


result.to_csv("final_churn_insights_table.csv",
              index=False, encoding="utf-8")
print("ðŸŽ‰ Final churn insights table generated successfully.")