import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score
)
from sklearn.linear_model import LogisticRegression

from xgboost import XGBClassifier
from catboost import CatBoostClassifier
import joblib

# =========================================================
# 1. LOAD & PREPARE DATA
# =========================================================
df = pd.read_csv("../data/processed/ravenstack_churn_ml_dataset_90day.csv")

target_col = "churn_flag_y"
y = df[target_col]

cols_to_drop = [
    target_col,
    "churn_flag_x",
    "account_id",
    "account_name",
    "feedback_text",
    "signup_date",
    "churn_date"
]
cols_to_drop = [c for c in cols_to_drop if c in df.columns]

X = df.drop(columns=cols_to_drop)

# One-hot encode (XGBoost needs this)
X_encoded = pd.get_dummies(X, drop_first=True).fillna(0)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y,
    test_size=0.3,
    stratify=y,
    random_state=42
)

# =========================================================
# 2. FEATURE SELECTION USING BASE XGBOOST
# =========================================================
scale_pos = (y_train == 0).sum() / (y_train == 1).sum()

xgb_base = XGBClassifier(
    n_estimators=400,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=1,
    scale_pos_weight=scale_pos,
    eval_metric="auc",
    random_state=42
)

xgb_base.fit(X_train, y_train)

importances = pd.Series(xgb_base.feature_importances_, index=X_train.columns)
selected_features = importances[importances > 0.002].index

X_train_fs = X_train[selected_features]
X_test_fs  = X_test[selected_features]

print(f"Selected {len(selected_features)} features out of {len(X_train.columns)}")

# =========================================================
# 3. FINAL MODELS (XGBOOST + CATBOOST)
# =========================================================
xgb_fs = XGBClassifier(
    n_estimators=600,
    learning_rate=0.2,
    max_depth=6,
    subsample=1.0,
    colsample_bytree=0.6,
    min_child_weight=1,
    scale_pos_weight=scale_pos,
    eval_metric="auc",
    random_state=42
)
xgb_fs.fit(X_train_fs, y_train)

cb_fs = CatBoostClassifier(
    iterations=600,
    learning_rate=0.05,
    depth=8,
    loss_function="Logloss",
    eval_metric="AUC",
    class_weights=[1, 4],
    verbose=False
)
cb_fs.fit(X_train_fs, y_train)

# =========================================================
# 4. SOFT-VOTING ENSEMBLE
# =========================================================
xgb_proba = xgb_fs.predict_proba(X_test_fs)[:, 1]
cb_proba  = cb_fs.predict_proba(X_test_fs)[:, 1]

ensemble_proba = (0.6 * xgb_proba) + (0.4 * cb_proba)

# =========================================================
# 5. PROBABILITY CALIBRATION
# =========================================================
xgb_train_proba = xgb_fs.predict_proba(X_train_fs)[:, 1]
cb_train_proba  = cb_fs.predict_proba(X_train_fs)[:, 1]
ensemble_train_proba = (0.6 * xgb_train_proba) + (0.4 * cb_train_proba)

calibrator = LogisticRegression()
calibrator.fit(ensemble_train_proba.reshape(-1, 1), y_train)

ensemble_proba_cal = calibrator.predict_proba(ensemble_proba.reshape(-1, 1))[:, 1]

# =========================================================
# 6. THRESHOLD TUNING (FINAL)
# =========================================================
thresholds = np.arange(0.01, 0.50, 0.01)
best_f1 = -1
best_threshold = 0.10

for t in thresholds:
    preds = (ensemble_proba_cal >= t).astype(int)
    f1 = f1_score(y_test, preds)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = t

print("\nBEST THRESHOLD =", round(best_threshold, 3))
print("Best F1 =", round(best_f1, 3))

# =========================================================
# 7. FINAL MODEL EVALUATION
# =========================================================
y_pred_final = (ensemble_proba_cal >= best_threshold).astype(int)

print("\n==== FINAL MODEL REPORT ====")
print("Threshold =", best_threshold)
print(classification_report(y_test, y_pred_final))
print("ROC-AUC:", roc_auc_score(y_test, ensemble_proba_cal))
print(confusion_matrix(y_test, y_pred_final))

# =========================================================
# 8. SAVE FINAL MODELS
# =========================================================
joblib.dump(xgb_fs, "../models/xgb_final.pkl")
joblib.dump(cb_fs, "../models/catboost_final.pkl")
joblib.dump(calibrator, "../models/calibration_final.pkl")
joblib.dump(list(selected_features), "../models/selected_features.pkl")

print("\nðŸŽ‰ Final models saved successfully.")